# getting all july tweets with hashtags
A = LOAD '/data/twitter/tweets2009-07.csv' USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY post MATCHES '.*#[^\\s]+.*';
STORE B INTO 'output/julyTaggedTweets';

# preprocess tweets (lowercase, remove odd characters)
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file clean_tweet.py \
     -mapper clean_tweet.py \
     -input output/julyTaggedTweets/part* \
     -output output/julyTaggedTweetsClean

# grab all hashtag -> term pairs <hashtag, term, date, user>
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file hashtag_terms.py \
     -mapper hashtag_terms.py \
     -input output/julyTaggedTweetsClean/part* \
     -output output/julyTagTerms

# grab all user-date-hashtag pairs <user, date, hashtag>
A = LOAD 'output/julyTaggedTweetsClean/part*' USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FOREACH A GENERATE user, date, FLATTEN(TOKENIZE(post)) as token;
C = FILTER B BY token MATCHES '(#[^\\s]+)';
STORE C INTO 'output/julyUserHashUses';

# grab counts of hashtags used by hashtag <hashtag, frequency>
A = LOAD 'output/julyUserHashUses/part*' USING PigStorage('\t') AS (user:chararray, date:chararray, hashtag:chararray);
B = GROUP A BY hashtag;
C = FOREACH B GENERATE group, COUNT(A) AS hash_count;
D = ORDER C BY hash_count DESC;
STORE D INTO 'output/julyTagUseCounts';

# grabbing co-occurrences of top 200 tags
#TODO: avoid duplicate co-occurrences
tag_counts = LOAD 'output/julyTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 200;
tag_terms = LOAD 'output/julyTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
STORE ttt_counts INTO 'output/julyTop200TagCooccurCounts';


####################
August data building
####################
# getting all august tweets with hashtags
A = LOAD '/data/twitter/tweets2009-08.csv' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY post MATCHES '.*#[^\\s]+.*';
STORE B INTO 'output/augTaggedTweets';

# preprocess tweets (lowercase, remove odd characters)
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file clean_tweet.py \
     -mapper clean_tweet.py \
     -input output/augTaggedTweets/part* \
     -output output/augTaggedTweetsClean

# grab all hashtag -> term pairs <hashtag, term, date, user>
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file hashtag_terms.py \
     -mapper hashtag_terms.py \
     -input output/augTaggedTweetsClean/part* \
     -output output/augTagTerms



# DEBUGGING: grab all 8/15/2009 tweets
A = LOAD 'output/augTaggedTweets/part*' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY SUBSTRING(date, 0, 10) == '2009-08-15';
STORE B INTO 'output/augTaggedTweets0815';

# DEBUGGING: sort <hashtag, term, date, user> pairs by date, hashtag
A = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
B = ORDER A BY date, hashtag;
STORE B INTO 'output/augTagTermsSort';

# DEBUGGING: grab all 8/15/2009 <hashtag, term, date, user> pairs
A = LOAD 'output/augTagTermsSort/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
B = FILTER A BY SUBSTRING(date, 0, 10) == '2009-08-15';
STORE B INTO 'output/augTagTerms0815';




# grab all user-date-hashtag pairs <user, date, hashtag>
A = LOAD 'output/augTaggedTweetsClean/part*' USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FOREACH A GENERATE user, date, FLATTEN(TOKENIZE(post)) as token;
C = FILTER B BY token MATCHES '(#[^\\s]+)';
STORE C INTO 'output/augUserHashUses';

# grab counts of hashtags used by hashtag <hashtag, frequency>
A = LOAD 'output/augUserHashUses/part*' USING PigStorage('\t') AS (user:chararray, date:chararray, hashtag:chararray);
B = GROUP A BY hashtag;
C = FOREACH B GENERATE group, COUNT(A) AS hash_count;
D = ORDER C BY hash_count DESC;
STORE D INTO 'output/augTagUseCounts';




######## BROKEN ###########
# grab all user-date-hashtag pairs <user, date, hashtag>
A = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
B = FILTER A BY term MATCHES '(#[^\\s]+)';
C = FOREACH B GENERATE user, date, hashtag;
STORE C INTO 'output/augUserHashUses';

# grab counts of hashtags used by hashtag <hashtag, frequency>
A = LOAD 'output/augUserHashUses/part*' USING PigStorage('\t') AS (user:chararray, date:chararray, hashtag:chararray);
B = GROUP A BY hashtag;
C = FOREACH B GENERATE group, COUNT(A) AS hash_count;
D = ORDER C BY hash_count DESC;
STORE D INTO 'output/augTagUseCounts';
####### END BROKEN ##########






# grabbing co-occurrences of top 200 tags
#TODO: avoid duplicate co-occurrences
tag_counts = LOAD 'output/augTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 200;
tag_terms = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/augTop200TagCooccurCounts';


# grabbing co-occurrences of top 5000 tags
#TODO: avoid duplicate co-occurrences
tag_counts = LOAD 'output/augTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 5000;
tag_terms = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/augTop5kTagCooccurCounts';



# grabbing data for use
cd data
mkdir augUserHashUses
hadoop fs -copyToLocal output/augUserHashUses/part* augUserHashUses
tar -czf augUserHashUses.tgz augUserHashUses
hadoop fs -cat output/augTop200TagCooccurCounts/part* > augTop200TagCooccurCounts.txt 
tar -czf augTop200TagCooccurCounts.tgz augTop200TagCooccurCounts.txt















# playground for grabbing co-occurrences of top 200 tags 
tag_counts = LOAD 'test/tagUseCounts.txt' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 3;
tag_terms = LOAD 'test/tagTerms.txt' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
jnd = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP jnd BY ($0, $1);
ttt_counts = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(jnd);
DUMP ttt_counts;


top_tag_tags
(#fb,#something,2009-01-01 04:04:04,dolan,#fb,309)
(#fb,#running,2009-01-01 04:04:04,dolan,#fb,309)
(#lost,#losting,2009-01-01 04:04:04,dolan,#lost,29)
(#lost,#fb,2009-01-01 04:04:04,dolan,#lost,29)

tag_tags
(#something,#testing,2009-01-01 04:04:04,adsf)
(#fb,#something,2009-01-01 04:04:04,dolan)
(#fb,#running,2009-01-01 04:04:04,dolan)
(#guest,#going,2009-01-01 04:04:04,dsfa)
(#lost,#losting,2009-01-01 04:04:04,dolan)
(#lost,#fb,2009-01-01 04:04:04,dolan)

top_tags
(#fb,309)
(#lost,29)
(#term,30)

