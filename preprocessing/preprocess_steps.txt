-- getting all july tweets with hashtags
A = LOAD 'twitter/*' USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY post MATCHES '.*#[^\\s]+.*' AND SUBSTRING(date, 0, 7) == '2009-07';
STORE B INTO 'output/july/TaggedTweets';

# preprocess tweets (lowercase, remove odd characters)
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file clean_tweet.py \
     -mapper clean_tweet.py \
     -input output/july/TaggedTweets/part* \
     -output output/july/TaggedTweetsClean

# grab all hashtag -> term pairs <hashtag, term, date, user>
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file hashtag_terms.py \
     -mapper hashtag_terms.py \
     -input output/july/TaggedTweetsClean/part* \
     -output output/july/TagTerms

# grab all user-date-hashtag pairs <user, date, hashtag>
A = LOAD 'output/julyTaggedTweetsClean/part*' USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FOREACH A GENERATE user, date, FLATTEN(TOKENIZE(post)) as token;
C = FILTER B BY token MATCHES '(#[^\\s]+)';
STORE C INTO 'output/july/UserHashUses';

# grab counts of hashtags used by hashtag <hashtag, frequency>
A = LOAD 'output/julyUserHashUses/part*' USING PigStorage('\t') AS (user:chararray, date:chararray, hashtag:chararray);
B = GROUP A BY hashtag;
C = FOREACH B GENERATE group, COUNT(A) AS hash_count;
D = ORDER C BY hash_count DESC;
STORE D INTO 'output/july/TagUseCounts';

# grabbing co-occurrences of top 200 tags
#TODO: avoid duplicate co-occurrences
tag_counts = LOAD 'output/july/TagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 200;
tag_terms = LOAD 'output/july/TagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
STORE ttt_counts INTO 'output/july/Top200TagCooccurCounts';


######## 100k random tweets' terms w/ freq > 5 ##########
-- loading random id adder
DEFINE rand_id_adder `add_rand_id_to_tweet.py`
    SHIP('/home/dol/eecs545_twitter/preprocessing/add_rand_id_to_tweet.py');

-- grabbing full ngram signal results
tweets = LOAD '/user/dol/output/july/TaggedTweetsClean/part*' 
         AS (date:chararray, user:chararray, post:chararray);

-- search results through streaming job
tweets_w_id = STREAM tweets THROUGH rand_id_adder 
          AS (id:chararray, date:chararray, user:chararray, post:chararray);
          
-- sorting results and returning just ngram and correlation
all_rand_tweets = ORDER tweets_w_id by id;
limit_rand_tweets = LIMIT all_rand_tweets 100000;
rand_tweets = FOREACH limit_rand_tweets GENERATE date, user, post;


-- list of distinct terms in first 1mil tweets
tokened_tweets = FOREACH rand_tweets GENERATE FLATTEN(TOKENIZE(post)) as token;
term_groups = GROUP tokened_tweets BY token;
term_freqs = FOREACH term_groups GENERATE group AS term, COUNT(tokened_tweets) AS freq;
common_term_freqs = FILTER term_freqs BY freq >= 5;
common_terms = FOREACH common_term_freqs GENERATE term;
STORE common_terms INTO 'output/july/uniqueTermsRand100k';


-- loading tweet-terms-mapper
DEFINE tweet_terms_mapper `tweet_terms_mapper.py`
    SHIP('/home/dol/eecs545_twitter/preprocessing/tweet_terms_mapper.py')
    CACHE('/user/dol/output/july/uniqueTermsRand100k/part-r-00000#tweet_terms.txt'); 

-- storing tweet terms data
final = STREAM rand_tweets THROUGH tweet_terms_mapper 
        AS (date:chararray, user:chararray, term_ids:chararray, hashtag_ids:chararray);
STORE final INTO 'output/july/100kTweetTermsRand';
######## 100k random tweets' terms ##########









####################
August data building
####################
# getting all august tweets with hashtags
A = LOAD '/data/twitter/tweets2009-08.csv' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY post MATCHES '.*#[^\\s]+.*'
    AND SUBSTRING(date, 0, 7) == '2009-08';
STORE B INTO 'output/augTaggedTweets';

# preprocess tweets (lowercase, remove odd characters)
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file clean_tweet.py \
     -mapper clean_tweet.py \
     -input output/augTaggedTweets/part* \
     -output output/augTaggedTweetsClean

# grab all hashtag -> term pairs <hashtag, term, date, user>
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file hashtag_terms.py \
     -mapper hashtag_terms.py \
     -input output/augTaggedTweetsClean/part* \
     -output output/augTagTerms

# grab all user-date-hashtag pairs <user, date, hashtag>
A = LOAD 'output/augTaggedTweetsClean/part*' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FOREACH A GENERATE user, date, FLATTEN(TOKENIZE(post)) as token;
C = FILTER B BY token MATCHES '(#[^\\s]+)' AND SIZE(token) > 2
    AND token != '#fb' AND token != '#ff';
STORE C INTO 'output/augUserHashUses';

# grab counts of hashtags used by hashtag <hashtag, frequency>
A = LOAD 'output/augUserHashUses/part*' 
    USING PigStorage('\t') AS (user:chararray, date:chararray, hashtag:chararray);
B = GROUP A BY hashtag;
C = FOREACH B GENERATE group, COUNT(A) AS hash_count;
D = ORDER C BY hash_count DESC;
STORE D INTO 'output/augTagUseCounts';

# grabbing co-occurrences of top 200 tags
tag_counts = LOAD 'output/augTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 200;
tag_terms = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/augTop200TagCooccurCounts';

# top 600
tag_counts = LOAD 'output/augTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 600;
tag_terms = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/augTop600TagCooccurCounts';

# top 1000
tag_counts = LOAD 'output/augTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 1000;
tag_terms = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/augTop1kTagCooccurCounts';

# top 2k
tag_counts = LOAD 'output/augTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 2000;
tag_terms = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/augTop2kTagCooccurCounts';

# grabbing co-occurrences of top 5000 tags
tag_counts = LOAD 'output/augTagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 5000;
tag_terms = LOAD 'output/augTagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/augTop5kTagCooccurCounts';


# grabbing data for use
cd data
mkdir augUserHashUses
hadoop fs -copyToLocal output/augUserHashUses/part* augUserHashUses
tar -czf augUserHashUses.tgz augUserHashUses
hadoop fs -cat output/augTagUseCounts/part* > augTagUseCounts.txt
tar -czf augTagUseCounts.tgz augTagUseCounts.txt
hadoop fs -cat output/augTop200TagCooccurCounts/part* > augTop200TagCooccurCounts.txt 
tar -czf augTop200TagCooccurCounts.tgz augTop200TagCooccurCounts.txt
hadoop fs -cat output/augTop5kTagCooccurCounts/part* > augTop5kTagCooccurCounts.txt 
tar -czf augTop5kTagCooccurCounts.tgz augTop5kTagCooccurCounts.txt










##########################################################
################## JULY + AUG DOWNLOADS ##################
# getting all august tweets with hashtags
A = LOAD 'twitter/'
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY post MATCHES '.*#[^\\s]+.*'
    AND (SUBSTRING(date, 0, 7) == '2009-07' OR SUBSTRING(date, 0, 7) == '2009-08');
STORE B INTO 'output/julyAug/TaggedTweets';

# preprocess tweets (lowercase, remove odd characters)
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file clean_tweet.py \
     -mapper clean_tweet.py \
     -input output/julyAug/TaggedTweets/part* \
     -output output/julyAug/TaggedTweetsClean

# grab all hashtag -> term pairs <hashtag, term, date, user>
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file hashtag_terms.py \
     -mapper hashtag_terms.py \
     -input output/julyAug/TaggedTweetsClean/part* \
     -output output/julyAug/TagTerms

# grab all user-date-hashtag pairs <user, date, hashtag>
A = LOAD 'output/julyAug/TaggedTweetsClean/part*' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FOREACH A GENERATE user, date, FLATTEN(TOKENIZE(post)) as token;
C = FILTER B BY token MATCHES '(#[^\\s]+)' AND SIZE(token) > 2
    AND token != '#fb' AND token != '#ff';
STORE C INTO 'output/julyAug/UserHashUses';

# grab counts of hashtags used by hashtag <hashtag, frequency>
A = LOAD 'output/julyAug/UserHashUses/part*' 
    USING PigStorage('\t') AS (user:chararray, date:chararray, hashtag:chararray);
B = GROUP A BY hashtag;
C = FOREACH B GENERATE group, COUNT(A) AS hash_count;
D = ORDER C BY hash_count DESC;
STORE D INTO 'output/julyAug/TagUseCounts';

# grabbing co-occurrences of top 200/600/1000/2000/5000 tags
tag_counts = LOAD 'output/julyAug/TagUseCounts/part*' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 200;
tag_terms = LOAD 'output/julyAug/TagTerms/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
top_tag_tags = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP top_tag_tags BY ($0, $1);
ttt_counts_tmp = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(top_tag_tags);
ttt_counts = FOREACH ttt_counts_tmp GENERATE $0 AS toptag, $1 AS cotag, $2 AS freq;
ttt_counts_filtered = FILTER ttt_counts BY freq >= 5;
ttt_counts_final = ORDER ttt_counts_filtered BY toptag, freq DESC; 
STORE ttt_counts_final INTO 'output/julyAug/Top200TagCooccurCounts';


######## 100k non-random tweets' terms ##########
# grabbing 100k tweets
A = LOAD 'output/julyAug/TaggedTweetsClean/part*' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = LIMIT A 100000;   
STORE B INTO 'output/julyAug/100kTweets';

# list of distinct terms in first 100k tweets
A = LOAD 'output/julyAug/100kTweets/part*' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
C = FOREACH B GENERATE FLATTEN(TOKENIZE(post)) as token;
D = DISTINCT C;
STORE D INTO 'output/julyAug/uniqueTerms';

# grabbing all term ids for 100k tweets
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file tweet_terms.txt \
     -file tweet_terms_mapper.py \
     -mapper tweet_terms_mapper.py \
     -input output/julyAug/100kTweets/part* \
     -output output/julyAug/100kTweetTerms
######## 100k non-random tweets' terms ##########



######## 100k random tweets' terms ##########
-- loading random id adder
DEFINE rand_id_adder `add_rand_id_to_tweet.py`
    SHIP('/home/dol/eecs545_twitter/preprocessing/add_rand_id_to_tweet.py');

-- grabbing full ngram signal results
tweets = LOAD '/user/dol/output/julyAug/TaggedTweetsClean/part*' 
         AS (date:chararray, user:chararray, post:chararray);

-- search results through streaming job
tweets_w_id = STREAM tweets THROUGH rand_id_adder 
          AS (id:chararray, date:chararray, user:chararray, post:chararray);
          
-- sorting results and returning just ngram and correlation
all_rand_tweets = ORDER tweets_w_id by id;
limit_rand_tweets = LIMIT all_rand_tweets 100000;
rand_tweets = FOREACH limit_rand_tweets GENERATE date, user, post;


-- list of distinct terms in first 100k tweets
tokened_tweets = FOREACH rand_tweets GENERATE FLATTEN(TOKENIZE(post)) as token;
unique_terms = DISTINCT tokened_tweets;
STORE unique_terms INTO 'output/julyAug/uniqueTermsRand';


-- loading tweet-terms-mapper
DEFINE tweet_terms_mapper `tweet_terms_mapper.py`
    SHIP('/home/dol/eecs545_twitter/preprocessing/tweet_terms_mapper.py')
    CACHE('/user/dol/output/julyAug/uniqueTermsRand/part-r-00000#tweet_terms.txt'); 

-- storing tweet terms data
final = STREAM rand_tweets THROUGH tweet_terms_mapper 
        AS (date:chararray, user:chararray, term_ids:chararray, hashtag_ids:chararray);
STORE final INTO 'output/julyAug/100kTweetTermsRand';
######## 100k random tweets' terms ##########








######## 1mil random tweets' terms w/ freq > 5 ##########
-- loading random id adder
DEFINE rand_id_adder `add_rand_id_to_tweet.py`
    SHIP('/home/dol/eecs545_twitter/preprocessing/add_rand_id_to_tweet.py');

-- grabbing full ngram signal results
tweets = LOAD '/user/dol/output/julyAug/TaggedTweetsClean/part*' 
         AS (date:chararray, user:chararray, post:chararray);

-- search results through streaming job
tweets_w_id = STREAM tweets THROUGH rand_id_adder 
          AS (id:chararray, date:chararray, user:chararray, post:chararray);
          
-- sorting results and returning just ngram and correlation
all_rand_tweets = ORDER tweets_w_id by id;
limit_rand_tweets = LIMIT all_rand_tweets 1000000;
rand_tweets = FOREACH limit_rand_tweets GENERATE date, user, post;


-- list of distinct terms in first 1mil tweets
tokened_tweets = FOREACH rand_tweets GENERATE FLATTEN(TOKENIZE(post)) as token;
term_groups = GROUP tokened_tweets BY token;
term_freqs = FOREACH term_groups GENERATE group AS term, COUNT(tokened_tweets) AS freq;
common_term_freqs = FILTER term_freqs BY freq >= 5;
common_terms = FOREACH common_term_freqs GENERATE term;
STORE common_terms INTO 'output/julyAug/uniqueTermsRand1M';


-- loading tweet-terms-mapper
DEFINE tweet_terms_mapper `tweet_terms_mapper.py`
    SHIP('/home/dol/eecs545_twitter/preprocessing/tweet_terms_mapper.py')
    CACHE('/user/dol/output/julyAug/uniqueTermsRand1M/part-r-00000#tweet_terms.txt'); 

-- storing tweet terms data
final = STREAM rand_tweets THROUGH tweet_terms_mapper 
        AS (date:chararray, user:chararray, term_ids:chararray, hashtag_ids:chararray);
STORE final INTO 'output/julyAug/1MTweetTermsRand';
######## 1mil random tweets' terms ##########






# grabbing julyAug data
mkdir -p julyAug/UserHashUses
hadoop fs -copyToLocal output/julyAug/UserHashUses/part* julyAug/UserHashUses
tar -czf julyAug/UserHashUses.tgz julyAug/UserHashUses
hadoop fs -cat output/julyAug/TagUseCounts/part* > julyAug/TagUseCounts.txt
tar -czf julyAug/TagUseCounts.tgz julyAug/TagUseCounts.txt
hadoop fs -cat output/julyAug/Top200TagCooccurCounts/part* > julyAug/Top200TagCooccurCounts.txt 
tar -czf julyAug/Top200TagCooccurCounts.tgz julyAug/Top200TagCooccurCounts.txt
hadoop fs -cat output/julyAug/Top600TagCooccurCounts/part* > julyAug/Top600TagCooccurCounts.txt 
tar -czf julyAug/Top600TagCooccurCounts.tgz julyAug/Top600TagCooccurCounts.txt
hadoop fs -cat output/julyAug/Top1kTagCooccurCounts/part* > julyAug/Top1kTagCooccurCounts.txt 
tar -czf julyAug/Top1kTagCooccurCounts.tgz julyAug/Top1kTagCooccurCounts.txt
hadoop fs -cat output/julyAug/Top2kTagCooccurCounts/part* > julyAug/Top2kTagCooccurCounts.txt 
tar -czf julyAug/Top2kTagCooccurCounts.tgz julyAug/Top2kTagCooccurCounts.txt
hadoop fs -cat output/julyAug/Top5kTagCooccurCounts/part* > julyAug/Top5kTagCooccurCounts.txt 
tar -czf julyAug/Top5kTagCooccurCounts.tgz julyAug/Top5kTagCooccurCounts.txt

#BLUSTER COPY
mkdir -p ~/data/mlproj/julyAug/UserHashUses
cd ~/data/mlproj
cp -r ~/hdfs/output/julyAug/UserHashUses/part* julyAug/UserHashUses
tar -czf julyAug/UserHashUses.tgz julyAug/UserHashUses
cat ~/hdfs/output/julyAug/TagUseCounts/part* > julyAug/TagUseCounts.txt
tar -czf julyAug/TagUseCounts.tgz julyAug/TagUseCounts.txt
cat ~/hdfs/output/julyAug/Top200TagCooccurCounts/part* > julyAug/Top200TagCooccurCounts.txt 
tar -czf julyAug/Top200TagCooccurCounts.tgz julyAug/Top200TagCooccurCounts.txt
cat ~/hdfs/output/julyAug/Top600TagCooccurCounts/part* > julyAug/Top600TagCooccurCounts.txt 
tar -czf julyAug/Top600TagCooccurCounts.tgz julyAug/Top600TagCooccurCounts.txt
cat ~/hdfs/output/julyAug/Top1kTagCooccurCounts/part* > julyAug/Top1kTagCooccurCounts.txt 
tar -czf julyAug/Top1kTagCooccurCounts.tgz julyAug/Top1kTagCooccurCounts.txt
cat ~/hdfs/output/julyAug/Top2kTagCooccurCounts/part* > julyAug/Top2kTagCooccurCounts.txt 
tar -czf julyAug/Top2kTagCooccurCounts.tgz julyAug/Top2kTagCooccurCounts.txt
cat ~/hdfs/output/julyAug/Top5kTagCooccurCounts/part* > julyAug/Top5kTagCooccurCounts.txt 
tar -czf julyAug/Top5kTagCooccurCounts.tgz julyAug/Top5kTagCooccurCounts.txt

cat ~/hdfs/output/julyAug/100kTweetTerms/part* > julyAug/100kTweetTerms.txt
tar -czf julyAug/100kTweetTerms.tgz julyAug/100kTweetTerms.txt

cat ~/hdfs/output/julyAug/100kTweetTermsRand/part* > julyAug/100kTweetTermsRand.txt
tar -czf julyAug/100kTweetTerms.tgz julyAug/100kTweetTerms.txt


17,706,485
# grabbing count of tweets with hashtags (result: 15,802,895 / 178,411,396)
A = LOAD 'output/julyAug/TaggedTweetsClean/part*';
B = GROUP A ALL;
C = FOREACH B GENERATE COUNT(A);
DUMP C;

A = LOAD 'twitter/'
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY post MATCHES '.*#[^\\s]+.*'
    AND (SUBSTRING(date, 0, 7) == '2009-07' OR SUBSTRING(date, 0, 7) == '2009-08');
STORE B INTO 'output/julyAug/TaggedTweets';

A = LOAD 'twitter/'
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = filter A by INDEXOF(post, '#') != -1
    AND (SUBSTRING(date, 0, 7) == '2009-07' OR SUBSTRING(date, 0, 7) == '2009-08');
C = group B all;
D = foreach C generate COUNT(B);
DUMP D;









########### IGNORE #############
# DEBUGGING: grab <hashtag, term, date, user> for 8/15
hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar \
     -D mapred.reduce.tasks=0 \
     -file hashtag_terms.py \
     -mapper hashtag_terms.py \
     -input output/augTaggedTweets0815/part* \
     -output output/augTagTerms0815b

# DEBUGGING: grab all 8/15/2009 tweets
A = LOAD 'output/augTaggedTweetsClean/part*' 
    USING PigStorage('\t') AS (date:chararray, user:chararray, post:chararray);
B = FILTER A BY SUBSTRING(date, 0, 10) == '2009-08-15';
STORE B INTO 'output/augTaggedTweets0815';

# DEBUGGING: sort <hashtag, term, date, user> pairs by date, hashtag
A = LOAD 'output/augTagTerms/part*' 
    USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
B = ORDER A BY date, hashtag;
STORE B INTO 'output/augTagTermsSort';

# DEBUGGING: grab all 8/15/2009 <hashtag, term, date, user> pairs
A = LOAD 'output/augTagTerms/part*' 
    USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
B = FILTER A BY SUBSTRING(date, 0, 10) == '2009-08-15';
STORE B INTO 'output/augTagTerms0815';

# DEBUGGING: grabbing unique entries
A = LOAD 'output/augTagTerms0815/part*' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
B = FILTER A BY term MATCHES '(#[^\\s]+)';
C = FOREACH B GENERATE user, date, hashtag;
STORE C INTO 'output/augUserHashUses0815';
######### END IGNORE #############






# playground for grabbing co-occurrences of top 200 tags 
tag_counts = LOAD 'test/tagUseCounts.txt' USING PigStorage('\t') AS (hashtag:chararray, hash_count:long);
tag_counts_ordered = ORDER tag_counts BY hash_count DESC;
top_tags = LIMIT tag_counts_ordered 3;
tag_terms = LOAD 'test/tagTerms.txt' USING PigStorage('\t') AS (hashtag:chararray, term:chararray, date:chararray, user:chararray);
tag_tags = FILTER tag_terms BY term MATCHES '^#.*$';
jnd = JOIN tag_tags BY hashtag, top_tags by hashtag USING 'skewed';
ttt_group = GROUP jnd BY ($0, $1);
ttt_counts = FOREACH ttt_group GENERATE FLATTEN(group), COUNT(jnd);
DUMP ttt_counts;


top_tag_tags
(#fb,#something,2009-01-01 04:04:04,dolan,#fb,309)
(#fb,#running,2009-01-01 04:04:04,dolan,#fb,309)
(#lost,#losting,2009-01-01 04:04:04,dolan,#lost,29)
(#lost,#fb,2009-01-01 04:04:04,dolan,#lost,29)

tag_tags
(#something,#testing,2009-01-01 04:04:04,adsf)
(#fb,#something,2009-01-01 04:04:04,dolan)
(#fb,#running,2009-01-01 04:04:04,dolan)
(#guest,#going,2009-01-01 04:04:04,dsfa)
(#lost,#losting,2009-01-01 04:04:04,dolan)
(#lost,#fb,2009-01-01 04:04:04,dolan)

top_tags
(#fb,309)
(#lost,29)
(#term,30)

