% USAGE: copy contents of this file into futureWorkConclusion.tex


\section{Future Work}

\subsection{Clustering}
We have shown that our clustering method has promise; we believe the method could be further improved. During the preprocessing step illustrated in Figure 1 the undirected graph is transformed to a directed graph that has the properties of a Markov chain. It would therefore be possible to perform a Markov Cluster Algorithm as proposed by Dongen \cite{Dongen2000}. It is possible that this algorithm would be less affected by noise, which would reduce the need for the preprocessing step and allow a larger number of hashtags to be clustered. Parallelizing the preprocessing step could also allow us to cluster a larger number of hashtags.

There may be a better way to pick the number of clusters. Increasing K was meant to increase the number of topics the clusters represented. However, instead of splitting large clusters apart into smaller topics, our algorithm would often pull one or two hashtags out at a time. This was most likely because $k$-means was always done on the full graph. It could be better to first apply $k$-means to the whole set, and have $k$ be a small number. Then for each of these clusters, perform localized $k$-means. This method could avoid the issue of creating clusters that are too small in size. 

\subsection{Classification}
We discovered that the performance of classification algorithms improves vastly if we shorten the time frame from which the data is drawn. It would be worth exploring the effects of introducing temporal information into our feature vectors since the importance of some hashtags seems to be limited to specific times.

Extending the co-occurrence idea from clustering, it could be worth exploring clustering of non-hashtag terms using a similarly defined measure and performing classification using these clusters of terms. This would result in lower-dimensional feature vectors, possibly making learning algorithms tractable without the use of PCA. Using this co-occurrence data might also be useful in classification of documents from different domains.

\section{Conclusion}
With a growing, diverse membership and masses of new information being added daily \cite{Holt2011}, opportunities to find useful information from Twitter will also grow. We have presented two new approaches that help unlock this information. The first focuses on clustering hashtags into meaningful topics; the second uses these clusters to identify the topic of tweets through classification. We also present two novel similarity measures that could be useful beyond the scope of the methods we examined. 

While we are pleased with the preliminary findings of our clustering and classification methods, our project hit some bumps along the way, causing us to deviate from our initial path. Initially, we attempted to cluster hashtags based on string similarity and not on semantic similarities. We found that the clusters created using these metrics were not very meaningful. This makes sense in hindsight since there are no rules of hashtag usage -- twitter users make a very diverse use of them, and there are many unique hashtags that are semantically similar while not being structurally similar. We also initially believed we would be able predict what a certain user is going to tweet about next based on his previously used hashtags. Instead, we found that many users preferred a very small set of unique hashtags, which made prediction more reliable, but much less meaningful. These roadblocks led us into our current project plan, which ended up being a promising direction for future work.


\section{Project Member Contributions}
All team members participated in data processing early in the project.  Most theory behind our project was a team effort.  Large scale data processing was handled by Dolan. Our preliminary research into hashtag usage prediction was co-developed by Akshay and Miller, working on naive Bayes and LDA, respectively.  Classifying hashtags based on tweets was primarily coded by Akshay and Miller, with Dolan assisting with execution. Exploration into and coding of different clustering techniques, and the development of the similarity measure and filtering was handled by Greg. Clustering analysis was done by Dolan and Greg. Miller worked on the trigram cosine similarity metric. Writing the paper was a team effort.